{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, ncol, nrow):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.x = 0  # 记录当前智能体位置的横坐标\n",
    "        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n",
    "\n",
    "    def step(self, action):  # 外部调用这个函数来改变当前位置\n",
    "        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n",
    "        next_state = self.y * self.ncol + self.x\n",
    "        reward = -1\n",
    "        done = False\n",
    "        if self.y == self.nrow - 1 and self.x > 0:  # 下一个位置在悬崖或者目标\n",
    "            done = True\n",
    "            if self.x != self.ncol - 1:\n",
    "                reward = -100\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):  # 回归初始状态,起点在左上角\n",
    "        self.x = 0\n",
    "        self.y = self.nrow - 1\n",
    "        return self.y * self.ncol + self.x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1649955038245,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "VZx5sBiD-vy9"
   },
   "outputs": [],
   "source": [
    "class DynaQ:\n",
    "    \"\"\" Dyna-Q算法 \"\"\"\n",
    "    def __init__(self,\n",
    "                 ncol,\n",
    "                 nrow,\n",
    "                 epsilon,\n",
    "                 alpha,\n",
    "                 gamma,\n",
    "                 n_planning,\n",
    "                 n_action=4):\n",
    "        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n",
    "        self.n_action = n_action  # 动作个数\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n",
    "\n",
    "        self.n_planning = n_planning  #执行Q-planning的次数, 对应1次Q-learning\n",
    "        self.model = dict()  # 环境模型\n",
    "\n",
    "    def take_action(self, state):  # 选取下一步的操作\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.n_action)\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[state])\n",
    "        return action\n",
    "\n",
    "    def q_learning(self, s0, a0, r, s1):\n",
    "        td_error = r + self.gamma * self.Q_table[s1].max(\n",
    "        ) - self.Q_table[s0, a0]\n",
    "        self.Q_table[s0, a0] += self.alpha * td_error\n",
    "\n",
    "    def update(self, s0, a0, r, s1):\n",
    "        self.q_learning(s0, a0, r, s1)\n",
    "        self.model[(s0, a0)] = r, s1  # 将数据添加到模型中\n",
    "        for _ in range(self.n_planning):  # Q-planning循环\n",
    "            # 随机选择曾经遇到过的状态动作对\n",
    "            (s, a), (r, s_) = random.choice(list(self.model.items()))\n",
    "            self.q_learning(s, a, r, s_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1649955038246,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "NOMaPWZS-vy9"
   },
   "outputs": [],
   "source": [
    "def DynaQ_CliffWalking(n_planning):\n",
    "    ncol = 12\n",
    "    nrow = 4\n",
    "    env = CliffWalkingEnv(ncol, nrow)\n",
    "    epsilon = 0.01\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    agent = DynaQ(ncol, nrow, epsilon, alpha, gamma, n_planning)\n",
    "    num_episodes = 300  # 智能体在环境中运行多少条序列\n",
    "\n",
    "    return_list = []  # 记录每一条序列的回报\n",
    "    for i in range(10):  # 显示10个进度条\n",
    "        # tqdm的进度条功能\n",
    "        with tqdm(total=int(num_episodes / 10),\n",
    "                  desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n",
    "                episode_return = 0\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n",
    "                    agent.update(state, action, reward, next_state)\n",
    "                    state = next_state\n",
    "                return_list.append(episode_return)\n",
    "                if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n",
    "                    pbar.set_postfix({\n",
    "                        'episode':\n",
    "                        '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                        'return':\n",
    "                        '%.3f' % np.mean(return_list[-10:])\n",
    "                    })\n",
    "                pbar.update(1)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "executionInfo": {
     "elapsed": 4145,
     "status": "ok",
     "timestamp": 1649955042382,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "26Jk6HKF-vy-",
    "outputId": "5bfb06b4-aacd-4fe1-ab92-c0e216e56dc6"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "n_planning_list = [0, 2, 20]\n",
    "for n_planning in n_planning_list:\n",
    "    print('Q-planning步数为：%d' % n_planning)\n",
    "    time.sleep(0.5)\n",
    "    return_list = DynaQ_CliffWalking(n_planning)\n",
    "    episodes_list = list(range(len(return_list)))\n",
    "    plt.plot(episodes_list,\n",
    "             return_list,\n",
    "             label=str(n_planning) + ' planning steps')\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('Dyna-Q on {}'.format('Cliff Walking'))\n",
    "plt.show()\n",
    "\n",
    "# Q-planning步数为：0\n",
    "\n",
    "# Iteration 0: 100%|██████████| 30/30 [00:00<00:00, 615.42it/s, episode=30,\n",
    "# return=-138.400]\n",
    "# Iteration 1: 100%|██████████| 30/30 [00:00<00:00, 1079.50it/s, episode=60,\n",
    "# return=-64.100]\n",
    "# Iteration 2: 100%|██████████| 30/30 [00:00<00:00, 1303.35it/s, episode=90,\n",
    "# return=-46.000]\n",
    "# Iteration 3: 100%|██████████| 30/30 [00:00<00:00, 1169.51it/s, episode=120,\n",
    "# return=-38.000]\n",
    "# Iteration 4: 100%|██████████| 30/30 [00:00<00:00, 1806.96it/s, episode=150,\n",
    "# return=-28.600]\n",
    "# Iteration 5: 100%|██████████| 30/30 [00:00<00:00, 2303.21it/s, episode=180,\n",
    "# return=-25.300]\n",
    "# Iteration 6: 100%|██████████| 30/30 [00:00<00:00, 2473.64it/s, episode=210,\n",
    "# return=-23.600]\n",
    "# Iteration 7: 100%|██████████| 30/30 [00:00<00:00, 2344.37it/s, episode=240,\n",
    "# return=-20.100]\n",
    "# Iteration 8: 100%|██████████| 30/30 [00:00<00:00, 1735.84it/s, episode=270,\n",
    "# return=-17.100]\n",
    "# Iteration 9: 100%|██████████| 30/30 [00:00<00:00, 2827.94it/s, episode=300,\n",
    "# return=-16.500]\n",
    "\n",
    "# Q-planning步数为：2\n",
    "\n",
    "# Iteration 0: 100%|██████████| 30/30 [00:00<00:00, 425.09it/s, episode=30,\n",
    "# return=-53.800]\n",
    "# Iteration 1: 100%|██████████| 30/30 [00:00<00:00, 655.71it/s, episode=60,\n",
    "# return=-37.100]\n",
    "# Iteration 2: 100%|██████████| 30/30 [00:00<00:00, 799.69it/s, episode=90,\n",
    "# return=-23.600]\n",
    "# Iteration 3: 100%|██████████| 30/30 [00:00<00:00, 915.34it/s, episode=120,\n",
    "# return=-18.500]\n",
    "# Iteration 4: 100%|██████████| 30/30 [00:00<00:00, 1120.39it/s, episode=150,\n",
    "# return=-16.400]\n",
    "# Iteration 5: 100%|██████████| 30/30 [00:00<00:00, 1437.24it/s, episode=180,\n",
    "# return=-16.400]\n",
    "# Iteration 6: 100%|██████████| 30/30 [00:00<00:00, 1366.79it/s, episode=210,\n",
    "# return=-13.400]\n",
    "# Iteration 7: 100%|██████████| 30/30 [00:00<00:00, 1457.62it/s, episode=240,\n",
    "# return=-13.200]\n",
    "# Iteration 8: 100%|██████████| 30/30 [00:00<00:00, 1743.68it/s, episode=270,\n",
    "# return=-13.200]\n",
    "# Iteration 9: 100%|██████████| 30/30 [00:00<00:00, 1699.59it/s, episode=300,\n",
    "# return=-13.500]\n",
    "\n",
    "# Q-planning步数为：20\n",
    "\n",
    "# Iteration 0: 100%|██████████| 30/30 [00:00<00:00, 143.91it/s, episode=30,\n",
    "# return=-18.500]\n",
    "# Iteration 1: 100%|██████████| 30/30 [00:00<00:00, 268.53it/s, episode=60,\n",
    "# return=-13.600]\n",
    "# Iteration 2: 100%|██████████| 30/30 [00:00<00:00, 274.53it/s, episode=90,\n",
    "# return=-13.000]\n",
    "# Iteration 3: 100%|██████████| 30/30 [00:00<00:00, 264.25it/s, episode=120,\n",
    "# return=-13.500]\n",
    "# Iteration 4: 100%|██████████| 30/30 [00:00<00:00, 263.58it/s, episode=150,\n",
    "# return=-13.500]\n",
    "# Iteration 5: 100%|██████████| 30/30 [00:00<00:00, 245.27it/s, episode=180,\n",
    "# return=-13.000]\n",
    "# Iteration 6: 100%|██████████| 30/30 [00:00<00:00, 257.16it/s, episode=210,\n",
    "# return=-22.000]\n",
    "# Iteration 7: 100%|██████████| 30/30 [00:00<00:00, 257.08it/s, episode=240,\n",
    "# return=-23.200]\n",
    "# Iteration 8: 100%|██████████| 30/30 [00:00<00:00, 261.12it/s, episode=270,\n",
    "# return=-13.000]\n",
    "# Iteration 9: 100%|██████████| 30/30 [00:00<00:00, 213.01it/s, episode=300,\n",
    "# return=-13.400]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第6章-Dyna-Q算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
